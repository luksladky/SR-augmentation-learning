{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to srthesis.callbacks,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "from nbdev import *\n",
    "%nbdev_default_export callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export_internal\n",
    "\n",
    "from fastai.basic_train import Learner\n",
    "from fastai.basic_data import DatasetType, DataBunch\n",
    "from fastai.vision import Image\n",
    "from fastai.vision.gan import GANLearner\n",
    "from fastai.basic_train import LearnerCallback\n",
    "from fastai.core import *\n",
    "from fastai.torch_core import *\n",
    "from threading import Thread, Event\n",
    "from queue import Queue\n",
    "import statistics\n",
    "import torchvision.utils as vutils\n",
    "from abc import ABC\n",
    "from time import sleep\n",
    "#This is an optional dependency in fastai.  Must install separately.\n",
    "try: from tensorboardX import SummaryWriter\n",
    "except: print(\"To use this tracker, please run 'pip install tensorboardx'. Also you must have Tensorboard running to see results\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorboard\n",
    "\n",
    "Provides convenient callbacks for Learners that write model images, metrics/losses, stats and histograms to Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Example usage (applies to any of the callbacks)__\n",
    "```\n",
    "proj_id = 'Colorize'\n",
    "tboard_path = Path('data/tensorboard/' + proj_id)\n",
    "learn.callback_fns.append(partial(GANTensorboardWriter, base_dir=tboard_path, name='GanLearner'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Async helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export_internal\n",
    "\n",
    "class TBWriteRequest(ABC):\n",
    "    \"A request object for Tensorboard writes.  Useful for queuing up and executing asynchronous writes.\"\n",
    "    def __init__(self, tbwriter: SummaryWriter, iteration:int):\n",
    "        super().__init__()\n",
    "        self.tbwriter = tbwriter\n",
    "        self.iteration = iteration\n",
    "\n",
    "    @abstractmethod\n",
    "    def write(self)->None: pass   \n",
    "\n",
    "# SummaryWriter writes tend to block quite a bit.  This gets around that and greatly boosts performance.\n",
    "# Not all tensorboard writes are using this- just the ones that take a long time.  Note that the \n",
    "# SummaryWriter does actually use a threadsafe consumer/producer design ultimately to write to Tensorboard, \n",
    "# so writes done outside of this async loop should be fine.\n",
    "class AsyncTBWriter():\n",
    "    \"Callback for GANLearners that writes to Tensorboard.  Extends LearnerTensorboardWriter and adds output image writes.\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.stop_request = Event()\n",
    "        self.queue = Queue()\n",
    "        self.thread = Thread(target=self._queue_processor, daemon=True)\n",
    "        self.thread.start()\n",
    "\n",
    "    def request_write(self, request: TBWriteRequest)->None:\n",
    "        \"Queues up an asynchronous write request to Tensorboard.\"\n",
    "        if self.stop_request.isSet(): return\n",
    "        self.queue.put(request)\n",
    "\n",
    "        \n",
    "    def _queue_processor(self)->None:\n",
    "        \"Processes queued up write requests asynchronously to Tensorboard.\"\n",
    "        while not self.stop_request.isSet():\n",
    "            while not self.queue.empty():\n",
    "                if self.stop_request.isSet(): return\n",
    "                request = self.queue.get()\n",
    "                request.write()\n",
    "            sleep(0.2)\n",
    "\n",
    "    #Provided this to stop thread explicitly or by context management (with statement) but thread should end on its own \n",
    "    # upon program exit, due to being a daemon.  So using this is probably unecessary.\n",
    "    def close(self)->None:\n",
    "        \"Stops asynchronous request queue processing thread.\"\n",
    "        self.stop_request.set()\n",
    "        self.thread.join()\n",
    "\n",
    "    # Nothing to do, thread already started.  Could start thread here to enforce use of context manager \n",
    "    # (but that sounds like a pain and a bit unweildy and unecessary for actual usage)\n",
    "    def __enter__(self): pass\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback): self.close()\n",
    "\n",
    "asyncTBWriter = AsyncTBWriter() \n",
    "\n",
    "class ModelImageSet():\n",
    "    \"Convenience object that holds the original, real(target) and generated versions of a single image fed to a model.\"\n",
    "    @staticmethod\n",
    "    def get_list_from_model(learn:Learner, ds_type:DatasetType, batch:Tuple)->[]:\n",
    "        \"Factory method to convert a batch of model images to a list of ModelImageSet.\"\n",
    "        image_sets = []\n",
    "        x,y = batch[0],batch[1]\n",
    "        preds = learn.pred_batch(ds_type=ds_type, batch=(x,y), reconstruct=True)  \n",
    "        for orig_px, real_px, gen in zip(x,y,preds):\n",
    "            orig, real = Image(px=orig_px), Image(px=real_px)\n",
    "            image_set = ModelImageSet(orig=orig, real=real, gen=gen)\n",
    "            image_sets.append(image_set)\n",
    "        return image_sets  \n",
    "\n",
    "    def __init__(self, orig:Image, real:Image, gen:Image): self.orig, self.real, self.gen = orig, real, gen\n",
    "\n",
    "class HistogramTBRequest(TBWriteRequest):\n",
    "    \"Request object for model histogram writes to Tensorboard.\"\n",
    "    def __init__(self, model:nn.Module, iteration:int, tbwriter:SummaryWriter, name:str):\n",
    "        super().__init__(tbwriter=tbwriter, iteration=iteration)\n",
    "        self.params = [(name, values.clone().detach().cpu()) for (name, values) in model.named_parameters()]\n",
    "        self.name = name\n",
    "\n",
    "    def _write_histogram(self, param_name:str, values)->None:\n",
    "        \"Writes single model histogram to Tensorboard.\"\n",
    "        tag = self.name + '/weights/' + param_name\n",
    "        self.tbwriter.add_histogram(tag=tag, values=values, global_step=self.iteration)\n",
    "\n",
    "    def write(self)->None:\n",
    "        \"Writes model histograms to Tensorboard.\"\n",
    "        for param_name, values in self.params: self._write_histogram(param_name=param_name, values=values)\n",
    "\n",
    "#If this isn't done async then this is sloooooow\n",
    "class HistogramTBWriter():\n",
    "    \"Writes model histograms to Tensorboard.\"\n",
    "    def __init__(self): super().__init__()\n",
    "\n",
    "    def write(self, model:nn.Module, iteration:int, tbwriter:SummaryWriter, name:str='model')->None:\n",
    "        \"Writes model histograms to Tensorboard.\"\n",
    "        request = HistogramTBRequest(model=model, iteration=iteration, tbwriter=tbwriter, name=name)\n",
    "        asyncTBWriter.request_write(request)\n",
    "\n",
    "class ModelStatsTBRequest(TBWriteRequest):\n",
    "    \"Request object for model gradient statistics writes to Tensorboard.\"\n",
    "    def __init__(self, model:nn.Module, iteration:int, tbwriter:SummaryWriter, name:str):\n",
    "        super().__init__(tbwriter=tbwriter, iteration=iteration)\n",
    "        self.gradients = [x.grad.clone().detach().cpu() for x in model.parameters() if x.grad is not None]\n",
    "        self.name = name\n",
    "\n",
    "    def _add_gradient_scalar(self, name:str, scalar_value)->None:\n",
    "        \"Writes a single scalar value for a gradient statistic to Tensorboard.\"\n",
    "        tag = self.name + '/gradients/' + name\n",
    "        self.tbwriter.add_scalar(tag=tag, scalar_value=scalar_value, global_step=self.iteration)\n",
    "\n",
    "    def _write_avg_norm(self, norms:[])->None:\n",
    "        \"Writes the average norm of the gradients to Tensorboard.\"\n",
    "        avg_norm = sum(norms)/len(self.gradients)\n",
    "        self._add_gradient_scalar('avg_norm', scalar_value=avg_norm)\n",
    "\n",
    "    def _write_median_norm(self, norms:[])->None:\n",
    "        \"Writes the median norm of the gradients to Tensorboard.\"\n",
    "        median_norm = statistics.median(norms)\n",
    "        self._add_gradient_scalar('median_norm', scalar_value=median_norm)\n",
    "\n",
    "    def _write_max_norm(self, norms:[])->None:\n",
    "        \"Writes the maximum norm of the gradients to Tensorboard.\"\n",
    "        max_norm = max(norms)\n",
    "        self._add_gradient_scalar('max_norm', scalar_value=max_norm)\n",
    "\n",
    "    def _write_min_norm(self, norms:[])->None:\n",
    "        \"Writes the minimum norm of the gradients to Tensorboard.\"\n",
    "        min_norm = min(norms)\n",
    "        self._add_gradient_scalar('min_norm', scalar_value=min_norm)\n",
    "\n",
    "    def _write_num_zeros(self)->None:\n",
    "        \"Writes the number of zeroes in the gradients to Tensorboard.\"\n",
    "        gradient_nps = [to_np(x.data) for x in self.gradients]\n",
    "        num_zeros = sum((np.asarray(x) == 0.0).sum() for x in gradient_nps)\n",
    "        self._add_gradient_scalar('num_zeros', scalar_value=num_zeros)\n",
    "\n",
    "    def _write_avg_gradient(self)->None:\n",
    "        \"Writes the average of the gradients to Tensorboard.\"\n",
    "        avg_gradient = sum(x.data.mean() for x in self.gradients)/len(self.gradients)\n",
    "        self._add_gradient_scalar('avg_gradient', scalar_value=avg_gradient)\n",
    "\n",
    "    def _write_median_gradient(self)->None:\n",
    "        \"Writes the median of the gradients to Tensorboard.\"\n",
    "        median_gradient = statistics.median(x.data.median() for x in self.gradients)\n",
    "        self._add_gradient_scalar('median_gradient', scalar_value=median_gradient)\n",
    "\n",
    "    def _write_max_gradient(self)->None:\n",
    "        \"Writes the maximum of the gradients to Tensorboard.\"\n",
    "        max_gradient = max(x.data.max() for x in self.gradients)\n",
    "        self._add_gradient_scalar('max_gradient', scalar_value=max_gradient)\n",
    "\n",
    "    def _write_min_gradient(self)->None:\n",
    "        \"Writes the minimum of the gradients to Tensorboard.\"\n",
    "        min_gradient = min(x.data.min() for x in self.gradients)\n",
    "        self._add_gradient_scalar('min_gradient', scalar_value=min_gradient)\n",
    "\n",
    "    def write(self)->None:\n",
    "        \"Writes model gradient statistics to Tensorboard.\"\n",
    "        if len(self.gradients) == 0: return\n",
    "        norms = [x.data.norm() for x in self.gradients]\n",
    "        self._write_avg_norm(norms=norms)\n",
    "        self._write_median_norm(norms=norms)\n",
    "        self._write_max_norm(norms=norms)\n",
    "        self._write_min_norm(norms=norms)\n",
    "        self._write_num_zeros()\n",
    "        self._write_avg_gradient()\n",
    "        self._write_median_gradient()\n",
    "        self._write_max_gradient()\n",
    "        self._write_min_gradient()\n",
    "\n",
    "class ModelStatsTBWriter():\n",
    "    \"Writes model gradient statistics to Tensorboard.\"\n",
    "    def write(self, model:nn.Module, iteration:int, tbwriter:SummaryWriter, name:str='model_stats')->None:\n",
    "        \"Writes model gradient statistics to Tensorboard.\"\n",
    "        request = ModelStatsTBRequest(model=model, iteration=iteration, tbwriter=tbwriter, name=name)\n",
    "        request.write()\n",
    "        #asyncTBWriter.request_write(request)\n",
    "\n",
    "class ImageTBRequest(TBWriteRequest):\n",
    "    \"Request object for model image output writes to Tensorboard.\"\n",
    "    def __init__(self, learn:Learner, batch:Tuple, iteration:int, tbwriter:SummaryWriter, ds_type:DatasetType):\n",
    "        super().__init__(tbwriter=tbwriter, iteration=iteration)\n",
    "        self.image_sets = ModelImageSet.get_list_from_model(learn=learn, batch=batch, ds_type=ds_type)\n",
    "        self.ds_type = ds_type\n",
    "\n",
    "    def _write_images(self, name:str, images:[Tensor])->None:\n",
    "        \"Writes list of images as tensors to Tensorboard.\"\n",
    "        tag = self.ds_type.name + ' ' + name\n",
    "        self.tbwriter.add_image(tag=tag, img_tensor=vutils.make_grid(images, normalize=True), global_step=self.iteration)\n",
    "\n",
    "    def _get_image_tensors(self)->([Tensor], [Tensor], [Tensor]):\n",
    "        \"Gets list of image tensors from lists of Image objects, as a tuple of original, generated and real(target) images.\"\n",
    "        orig_images, gen_images, real_images = [], [], []\n",
    "        for image_set in self.image_sets:\n",
    "            orig_images.append(image_set.orig.px)\n",
    "            gen_images.append(image_set.gen.px)\n",
    "            real_images.append(image_set.real.px) \n",
    "        return orig_images, gen_images, real_images  \n",
    "\n",
    "    def write(self)->None:\n",
    "        \"Writes original, generated and real(target) images to Tensorboard.\"\n",
    "        orig_images, gen_images, real_images = self._get_image_tensors()\n",
    "        self._write_images(name='orig images', images=orig_images)\n",
    "        self._write_images(name='gen images',  images=gen_images)\n",
    "        self._write_images(name='real images', images=real_images)\n",
    "\n",
    "#If this isn't done async then this is noticeably slower\n",
    "class ImageTBWriter():\n",
    "    \"Writes model image output to Tensorboard.\"\n",
    "    def __init__(self): super().__init__()\n",
    "\n",
    "    def write(self, learn:Learner, trn_batch:Tuple, val_batch:Tuple, iteration:int, tbwriter:SummaryWriter)->None:\n",
    "        \"Writes training and validation batch images to Tensorboard.\"\n",
    "        self._write_for_dstype(learn=learn, batch=val_batch, iteration=iteration, tbwriter=tbwriter, ds_type=DatasetType.Valid)\n",
    "        self._write_for_dstype(learn=learn, batch=trn_batch, iteration=iteration, tbwriter=tbwriter, ds_type=DatasetType.Train)\n",
    "\n",
    "    def _write_for_dstype(self, learn:Learner, batch:Tuple, iteration:int, tbwriter:SummaryWriter, ds_type:DatasetType)->None:\n",
    "        \"Writes batch images of specified DatasetType to Tensorboard.\"\n",
    "        request = ImageTBRequest(learn=learn, batch=batch, iteration=iteration, tbwriter=tbwriter, ds_type=ds_type)\n",
    "        request.write()\n",
    "        #asyncTBWriter.request_write(request)\n",
    "\n",
    "class GraphTBRequest(TBWriteRequest):\n",
    "    \"Request object for model histogram writes to Tensorboard.\"\n",
    "    def __init__(self, model:nn.Module, tbwriter:SummaryWriter, input_to_model:torch.Tensor):\n",
    "        super().__init__(tbwriter=tbwriter, iteration=0)\n",
    "        self.model,self.input_to_model = model,input_to_model\n",
    "\n",
    "    def write(self)->None:\n",
    "        \"Writes single model graph to Tensorboard.\"\n",
    "        self.tbwriter.add_graph(model=self.model, input_to_model=self.input_to_model)\n",
    "\n",
    "class GraphTBWriter():\n",
    "    \"Writes model network graph to Tensorboard.\"\n",
    "    def write(self, model:nn.Module, tbwriter:SummaryWriter, input_to_model:torch.Tensor)->None:\n",
    "        \"Writes model graph to Tensorboard.\"\n",
    "        request = GraphTBRequest(model=model, tbwriter=tbwriter, input_to_model=input_to_model)\n",
    "        request.write()\n",
    "        #asyncTBWriter.request_write(request)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic stats for critic pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nbdev_export` not found.\n"
     ]
    }
   ],
   "source": [
    "%nbdev_export\n",
    "class LearnerTensorboardWriter(LearnerCallback):\n",
    "    \"Broadly useful callback for Learners that writes to Tensorboard.  Writes model histograms, losses/metrics, and gradient stats.\"\n",
    "    def __init__(self, learn:Learner, tbwriter:SummaryWriter, loss_iters:int=1, hist_iters:int=500, stats_iters:int=100):\n",
    "        super().__init__(learn=learn)\n",
    "        self.tbwriter,self.loss_iters,self.hist_iters,self.stats_iters  = tbwriter,loss_iters,hist_iters,stats_iters\n",
    "        self.hist_writer = HistogramTBWriter()\n",
    "        self.stats_writer = ModelStatsTBWriter()\n",
    "        self.graph_writer = GraphTBWriter()\n",
    "        self.write_graph = False\n",
    "        self.data = None\n",
    "        self.metrics_root = '/metrics/'\n",
    "        self._update_batches_if_needed()\n",
    "        self.prev_iterations = 0\n",
    "        self.prev_epochs = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.prev_iterations = 0\n",
    "        self.prev_epochs = 0\n",
    "\n",
    "    def jump_to_epoch(self, epoch:int)->None:\n",
    "        n = len(self.learn.data.train_dl)\n",
    "        self.prev_iterations = n*epoch\n",
    "        print(f\"{n}, {n*epoch}\")\n",
    "        print(f\"TB: Continue from epoch {epoch}, iteration {self.prev_iterations}\")\n",
    "        \n",
    "    def _get_new_batch(self, ds_type:DatasetType)->Collection[Tensor]:\n",
    "        \"Retrieves new batch of DatasetType, and detaches it.\"\n",
    "        return self.learn.data.one_batch(ds_type=ds_type, detach=True, denorm=False, cpu=False)\n",
    "\n",
    "    def _update_batches_if_needed(self)->None:\n",
    "        \"one_batch function is extremely slow with large datasets.  This is caching the result as an optimization.\"\n",
    "        if self.learn.data.valid_dl is None: return # Running learning rate finder, so return\n",
    "        update_batches = self.data is not self.learn.data\n",
    "        if not update_batches: return\n",
    "        #print(\"_update_batches_if_needed\")\n",
    "        self.data = self.learn.data\n",
    "        self.trn_batch = self._get_new_batch(ds_type=DatasetType.Train)\n",
    "        self.val_batch = self._get_new_batch(ds_type=DatasetType.Valid)\n",
    "\n",
    "    def _write_model_stats(self, iteration:int)->None:\n",
    "        \"Writes gradient statistics to Tensorboard.\"\n",
    "        #print(\"_write_model_stats\")\n",
    "        self.stats_writer.write(model=self.learn.model, iteration=iteration, tbwriter=self.tbwriter)\n",
    "\n",
    "    def _write_training_loss(self, iteration:int, last_loss:Tensor)->None:\n",
    "        \"Writes training loss to Tensorboard.\"\n",
    "        #print(\"_write_training_loss\")\n",
    "        scalar_value = to_np(last_loss)\n",
    "        tag = self.metrics_root + 'train_loss'\n",
    "        self.tbwriter.add_scalar(tag=tag, scalar_value=scalar_value, global_step=iteration)\n",
    "\n",
    "    def _write_weight_histograms(self, iteration:int)->None:\n",
    "        \"Writes model weight histograms to Tensorboard.\"\n",
    "        self.hist_writer.write(model=self.learn.model, iteration=iteration, tbwriter=self.tbwriter)\n",
    "\n",
    "    def _write_scalar(self, name:str, scalar_value, iteration:int)->None:\n",
    "        \"Writes single scalar value to Tensorboard.\"\n",
    "        tag = self.metrics_root + name\n",
    "        self.tbwriter.add_scalar(tag=tag, scalar_value=scalar_value, global_step=iteration)\n",
    "\n",
    "    #TODO:  Relying on a specific hardcoded start_idx here isn't great.  Is there a better solution?\n",
    "    def _write_metrics(self, iteration:int, last_metrics:MetricsList, start_idx:int=2)->None:\n",
    "        \"Writes training metrics to Tensorboard.\"\n",
    "        recorder = self.learn.recorder\n",
    "        for i, name in enumerate(recorder.names[start_idx:]):\n",
    "            if last_metrics is None or len(last_metrics) < i+1: return\n",
    "            scalar_value = last_metrics[i]\n",
    "            self._write_scalar(name=name, scalar_value=scalar_value, iteration=iteration)\n",
    "\n",
    "    def on_train_begin(self, **kwargs: Any) -> None:\n",
    "        if self.write_graph:\n",
    "            self.graph_writer.write(model=self.learn.model, tbwriter=self.tbwriter,\n",
    "                                    input_to_model=next(iter(self.learn.data.dl(DatasetType.Single)))[0])\n",
    "\n",
    "    def on_batch_end(self, last_loss:Tensor, iteration:int, train:bool, **kwargs)->None:\n",
    "        \"Callback function that writes batch end appropriate data to Tensorboard.\"\n",
    "        it = iteration + self.prev_iterations\n",
    "        #print(f\"on_batch_end, loss:{last_loss}, train:{train}, it{iteration}/{it}\")\n",
    "        if it == 0 or not train: return\n",
    "        self._update_batches_if_needed()\n",
    "        if it % self.loss_iters == 0: \n",
    "            #print(\"_write_training_loss\")\n",
    "            self._write_training_loss(iteration=it, last_loss=last_loss)\n",
    "        if it % self.hist_iters == 0: \n",
    "            #print(\"_write_weight_histograms\")\n",
    "            self._write_weight_histograms(iteration=it)\n",
    "\n",
    "    # Doing stuff here that requires gradient info, because they get zeroed out afterwards in training loop\n",
    "    def on_backward_end(self, iteration:int, train:bool, **kwargs)->None:\n",
    "        \"Callback function that writes backward end appropriate data to Tensorboard.\"\n",
    "        if iteration == 0 and not train: return\n",
    "        self._update_batches_if_needed()\n",
    "        if iteration % self.stats_iters == 0: self._write_model_stats(iteration=iteration + self.prev_iterations)\n",
    "\n",
    "    def on_epoch_end(self, last_metrics:MetricsList, iteration:int, **kwargs)->None:\n",
    "        \"Callback function that writes epoch end appropriate data to Tensorboard.\"\n",
    "        self._write_metrics(iteration=iteration + self.prev_iterations, last_metrics=last_metrics)\n",
    "        self.prev_epochs += 1\n",
    "        \n",
    "    def on_train_end(self, iteration:int, **kwargs)->None:\n",
    "        \"Keep track of total numbers of iterations\"\n",
    "        #print(\"on_train_end\", iteration, self.prev_iterations)\n",
    "        self.prev_iterations += iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image generation for generator pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "\n",
    "class ImageGenTensorboardWriter(LearnerTensorboardWriter):\n",
    "    \"Callback for non-GAN image generating Learners that writes to Tensorboard.  Extends LearnerTensorboardWriter and adds output image writes.\"\n",
    "\n",
    "    def __init__(self, learn:Learner, tbwriter:SummaryWriter, loss_iters:int=25, hist_iters:int=500, stats_iters:int=100, \n",
    "                 visual_iters:int=100):\n",
    "        super().__init__(learn=learn, tbwriter=tbwriter, loss_iters=loss_iters, hist_iters=hist_iters, \n",
    "                         stats_iters=stats_iters)\n",
    "        self.visual_iters = visual_iters\n",
    "        self.img_gen_vis = ImageTBWriter()\n",
    "\n",
    "        \n",
    "    def _write_images(self, iteration:int)->None:\n",
    "        \"Writes model generated, original and real images to Tensorboard\"\n",
    "        self.img_gen_vis.write(learn=self.learn, trn_batch=self.trn_batch, val_batch=self.val_batch, iteration=iteration, \n",
    "                               tbwriter=self.tbwriter)\n",
    "\n",
    "    def on_batch_end(self, iteration:int, train:bool, **kwargs)->None:  \n",
    "        \"Callback function that writes batch end appropriate data to Tensorboard.\"\n",
    "        super().on_batch_end(iteration=iteration, train=train, **kwargs)\n",
    "        if iteration == 0 and not train: return\n",
    "        if iteration % self.visual_iters == 0: self._write_images(iteration=iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAN logging - basic stats and generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "# TODO:  We're overriding almost everything here.  Seems like a good idea to question that (\"is a\" vs \"has a\")\n",
    "class GANTensorboardWriter(LearnerTensorboardWriter):\n",
    "    \"Callback for GANLearners that writes to Tensorboard.  Extends LearnerTensorboardWriter and adds output image writes.\"\n",
    "    def __init__(self, learn:GANLearner, tbwriter:SummaryWriter, loss_iters:int=25, hist_iters:int=500, \n",
    "                stats_iters:int=100, visual_iters:int=100):\n",
    "        super().__init__(learn=learn, tbwriter = tbwriter, loss_iters=loss_iters, hist_iters=hist_iters, stats_iters=stats_iters)\n",
    "        self.visual_iters = visual_iters\n",
    "        self.img_gen_vis = ImageTBWriter()\n",
    "        self.gen_stats_updated = True\n",
    "        self.crit_stats_updated = True\n",
    "\n",
    "    def _write_weight_histograms(self, iteration:int)->None:\n",
    "        \"Writes model weight histograms to Tensorboard.\"\n",
    "        generator, critic = self.learn.gan_trainer.generator, self.learn.gan_trainer.critic\n",
    "        self.hist_writer.write(model=generator, iteration=iteration, tbwriter=self.tbwriter, name='generator')\n",
    "        self.hist_writer.write(model=critic,    iteration=iteration, tbwriter=self.tbwriter, name='critic')\n",
    "\n",
    "    def _write_gen_model_stats(self, iteration:int)->None:\n",
    "        \"Writes gradient statistics for generator to Tensorboard.\"\n",
    "        generator = self.learn.gan_trainer.generator\n",
    "        self.stats_writer.write(model=generator, iteration=iteration, tbwriter=self.tbwriter, name='gen_model_stats')\n",
    "        self.gen_stats_updated = True\n",
    "\n",
    "    def _write_critic_model_stats(self, iteration:int)->None:\n",
    "        \"Writes gradient statistics for critic to Tensorboard.\"\n",
    "        critic = self.learn.gan_trainer.critic\n",
    "        self.stats_writer.write(model=critic, iteration=iteration, tbwriter=self.tbwriter, name='crit_model_stats')\n",
    "        self.crit_stats_updated = True\n",
    "\n",
    "    def _write_model_stats(self, iteration:int)->None:\n",
    "        \"Writes gradient statistics to Tensorboard.\"\n",
    "        # We don't want to write stats when model is not iterated on and hence has zeroed out gradients\n",
    "        gen_mode = self.learn.gan_trainer.gen_mode\n",
    "        if gen_mode and not self.gen_stats_updated: self._write_gen_model_stats(iteration=iteration)\n",
    "        if not gen_mode and not self.crit_stats_updated: self._write_critic_model_stats(iteration=iteration)\n",
    "\n",
    "    def _write_training_loss(self, iteration:int, last_loss:Tensor)->None:\n",
    "        \"Writes training loss to Tensorboard.\"\n",
    "        recorder = self.learn.gan_trainer.recorder\n",
    "        if len(recorder.losses) == 0: return\n",
    "        scalar_value = to_np((recorder.losses[-1:])[0])\n",
    "        tag = self.metrics_root + 'train_loss'\n",
    "        self.tbwriter.add_scalar(tag=tag, scalar_value=scalar_value, global_step=iteration)\n",
    "\n",
    "    def _write_images(self, iteration:int)->None:\n",
    "        \"Writes model generated, original and real images to Tensorboard.\"\n",
    "        trainer = self.learn.gan_trainer\n",
    "        #TODO:  Switching gen_mode temporarily seems a bit hacky here.  Certainly not a good side-effect.  Is there a better way?\n",
    "        gen_mode = trainer.gen_mode\n",
    "        try:\n",
    "            trainer.switch(gen_mode=True)\n",
    "            self.img_gen_vis.write(learn=self.learn, trn_batch=self.trn_batch, val_batch=self.val_batch, \n",
    "                                    iteration=iteration, tbwriter=self.tbwriter)\n",
    "        finally: trainer.switch(gen_mode=gen_mode)\n",
    "\n",
    "    def on_batch_end(self, iteration:int, train:bool, **kwargs)->None:\n",
    "        \"Callback function that writes batch end appropriate data to Tensorboard.\"\n",
    "        super().on_batch_end(iteration=iteration, train=train, **kwargs)\n",
    "        if iteration == 0 and not train: return\n",
    "        if iteration % self.visual_iters == 0: self._write_images(iteration=iteration)\n",
    "\n",
    "    def on_backward_end(self, iteration:int, train:bool, **kwargs)->None:\n",
    "        \"Callback function that writes backward end appropriate data to Tensorboard.\"\n",
    "        if iteration == 0 and not train: return\n",
    "        self._update_batches_if_needed()\n",
    "        #TODO:  This could perhaps be implemented as queues of requests instead but that seemed like overkill. \n",
    "        # But I'm not the biggest fan of maintaining these boolean flags either... Review pls.\n",
    "        if iteration % self.stats_iters == 0: self.gen_stats_updated, self.crit_stats_updated = False, False\n",
    "        if not (self.gen_stats_updated and self.crit_stats_updated): self._write_model_stats(iteration=iteration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 0__template.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 11_div2k.ipynb.\n",
      "Converted 12_realsr.ipynb.\n",
      "Converted 20_metrics.ipynb.\n",
      "Converted 21_loss.ipynb.\n",
      "Converted 22_callbacks.ipynb.\n",
      "Converted 23_tensorboard.ipynb.\n",
      "Converted 31_generator_learner.ipynb.\n",
      "Converted 32_critic_learner.ipynb.\n",
      "Converted 41_generator_pretraining.ipynb.\n",
      "Converted 42_critic_pretraining.ipynb.\n",
      "Converted 43_gan_training.ipynb.\n",
      "Converted augmentations.ipynb.\n",
      "Converted sr reference.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
