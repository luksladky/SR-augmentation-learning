{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to srthesis.utils,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "from nbdev import *\n",
    "%nbdev_default_export utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export_internal\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from fastai.utils.mem import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def get_freer_gpu():\n",
    "    os.system('nvidia-smi -q -d Memory |grep -A4 GPU|grep Free >tmp')\n",
    "    memory_available = [int(x.split()[2]) for x in open('tmp', 'r').readlines()]\n",
    "    os.remove(\"tmp\")\n",
    "    return np.argmax(memory_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "def select_gpu():\n",
    "    freer_gpu = get_freer_gpu()\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = f'{freer_gpu}'\n",
    "    torch.cuda.set_device(0)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"GPUs:\"+str(torch.cuda.device_count()))\n",
    "    print(\"Selected GPU:\"+str(freer_gpu + 1))\n",
    "    print(\"Available memory:\" + str(gpu_mem_get_free_no_cache()) + \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs:2\n",
      "Selected GPU:2\n",
      "Available memory:11005MB\n"
     ]
    }
   ],
   "source": [
    "select_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%nbdev_export` not found.\n"
     ]
    }
   ],
   "source": [
    "%nbdev_export\n",
    "import sys\n",
    "def get_size(obj, seen=None):\n",
    "    \"\"\"Recursively finds size of objects\"\"\"\n",
    "\n",
    "    size = sys.getsizeof(obj)\n",
    "    if seen is None:\n",
    "        seen = set()\n",
    "\n",
    "    obj_id = id(obj)\n",
    "    if obj_id in seen:\n",
    "        return 0\n",
    "\n",
    "    # Important mark as seen *before* entering recursion to gracefully handle\n",
    "    # self-referential objects\n",
    "    seen.add(obj_id)\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        size += sum([get_size(v, seen) for v in obj.values()])\n",
    "        size += sum([get_size(k, seen) for k in obj.keys()])\n",
    "    elif hasattr(obj, '__dict__'):\n",
    "        size += get_size(obj.__dict__, seen)\n",
    "    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):\n",
    "        size += sum([get_size(i, seen) for i in obj])\n",
    "\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "\t\"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "\tassert(isinstance(size, torch.Size))\n",
    "\treturn \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "\t\"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "\timport gc\n",
    "\ttotal_size = 0\n",
    "\tfor obj in gc.get_objects():\n",
    "\t\ttry:\n",
    "\t\t\tif torch.is_tensor(obj):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t  \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  \" pinned\" if obj.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t  pretty_size(obj.size())))\n",
    "\t\t\t\t\ttotal_size += obj.numel()\n",
    "\t\t\telif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "\t\t\t\tif not gpu_only or obj.is_cuda:\n",
    "\t\t\t\t\tprint(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   type(obj.data).__name__, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" GPU\" if obj.is_cuda else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" grad\" if obj.requires_grad else \"\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   \" volatile\" if obj.volatile else \"\",\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t   pretty_size(obj.data.size())))\n",
    "\t\t\t\t\ttotal_size += obj.data.numel()\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tpass        \n",
    "\tprint(\"Total size:\", total_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 0__template.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 11_div2k.ipynb.\n",
      "Converted 12_realsr.ipynb.\n",
      "Converted 20_metrics.ipynb.\n",
      "Converted 21_loss.ipynb.\n",
      "Converted 22_callbacks.ipynb.\n",
      "Converted 23_tensorboard.ipynb.\n",
      "Converted 31_generator_learner.ipynb.\n",
      "Converted 32_critic_learner.ipynb.\n",
      "Converted 41_generator_pretraining.ipynb.\n",
      "Converted 42_critic_pretraining.ipynb.\n",
      "Converted 43_gan_training.ipynb.\n",
      "Converted EXPERIMENTS JOURNAL.ipynb.\n",
      "Converted augmentations.ipynb.\n",
      "Converted graphs-tests.ipynb.\n",
      "Converted sr reference.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
