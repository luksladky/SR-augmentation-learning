{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cells will be exported to srthesis.loss,\n",
      "unless a different module is specified after an export flag: `%nbdev_export special.module`\n"
     ]
    }
   ],
   "source": [
    "from nbdev import *\n",
    "%nbdev_default_export loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "> Feature loss used as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export_internal\n",
    "import fastai\n",
    "from fastai.vision import *\n",
    "from fastai.callbacks import *\n",
    "from torchvision.models import vgg16_bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature loss\n",
    "\n",
    "Feature loss (also perceptual loss) using activations from a pretrained model. In this case both target and prediction is run through a VGG16 model. We're taking activations from all layers just before MaxPool2d and comparing them with L1 loss. In addition there is a standard L1 pixel level loss and and Gramm matrices of activations ($ G=A^{T}A $) compared with L1 loss.\n",
    "\n",
    "$$ L = L_1 + L_{feat} + L_{Gramm} * 5\\times10^{3}$$\n",
    "\n",
    "Where \n",
    "$$ G_i = VGG_{16}(y)_i, \\hat G_i = VGG_{16}(\\hat y)_i$$\n",
    "\n",
    "$$ L_{Gramm} = \\sum_{a \\in A_{y}, \\hat a \\in A_{\\hat y}} L_1(G_i, \\hat G_i)$$ \n",
    "\n",
    "The constant weight of Gramm matrix is taken from a fast.ai course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export_internal\n",
    "\n",
    "def _gram_matrix(x):\n",
    "    n,c,h,w = x.size()\n",
    "    x = x.view(n, c, -1)\n",
    "    return (x @ x.transpose(1,2))/(c*h*w)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([5, 12, 22, 32, 42],\n",
       " [ReLU(inplace=True),\n",
       "  ReLU(inplace=True),\n",
       "  ReLU(inplace=True),\n",
       "  ReLU(inplace=True),\n",
       "  ReLU(inplace=True)])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_m = vgg16_bn(True).features.cuda().eval()\n",
    "requires_grad(vgg_m, False)\n",
    "\n",
    "blocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\n",
    "blocks, [vgg_m[i] for i in blocks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class FeatureLoss(nn.Module):\n",
    "    def __init__(self, layer_wgts=[20, 70, 10]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.m_feat = models.vgg16_bn(True).features.cuda().eval()\n",
    "        requires_grad(self.m_feat, False)\n",
    "        blocks = [\n",
    "            i - 1\n",
    "            for i, o in enumerate(children(self.m_feat))\n",
    "            if isinstance(o, nn.MaxPool2d)\n",
    "        ]\n",
    "        layer_ids = blocks[2:5]\n",
    "        self.loss_features = [self.m_feat[i] for i in layer_ids]\n",
    "        self.hooks = hook_outputs(self.loss_features, detach=False)\n",
    "        self.wgts = layer_wgts\n",
    "        self.metric_names = ['pixel'] + [f'feat_{i}' for i in range(len(layer_ids))]\n",
    "        self.base_loss = F.l1_loss\n",
    "\n",
    "    def _make_features(self, x, clone=False):\n",
    "        self.m_feat(x)\n",
    "        return [(o.clone() if clone else o) for o in self.hooks.stored]\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        out_feat = self._make_features(target, clone=True)\n",
    "        in_feat = self._make_features(input)\n",
    "        self.feat_losses = [self.base_loss(input, target)]\n",
    "        self.feat_losses += [\n",
    "            self.base_loss(f_in, f_out) * w\n",
    "            for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)\n",
    "        ]\n",
    "        self.feat_losses += [self.base_loss(_gram_matrix(f_in), _gram_matrix(f_out))*w**2 * 5e3\n",
    "                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n",
    "\n",
    "        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n",
    "        return sum(self.feat_losses)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.hooks.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "class L1Loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.metric_names = ['l1 loss']\n",
    "        self.base_loss = F.l1_loss\n",
    "\n",
    "\n",
    "    def forward(self, input, target):\n",
    "       \n",
    "        self.feat_losses = [self.base_loss(input, target)]\n",
    "        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n",
    "        return sum(self.feat_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use as `feat_loss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "feat_loss = FeatureLoss([5,15,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%nbdev_export\n",
    "l1_loss = L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_utils.ipynb.\n",
      "Converted 0__template.ipynb.\n",
      "Converted 10_data.ipynb.\n",
      "Converted 11_div2k.ipynb.\n",
      "Converted 12_realsr.ipynb.\n",
      "Converted 20_metrics.ipynb.\n",
      "Converted 21_loss.ipynb.\n",
      "Converted 22_callbacks.ipynb.\n",
      "Converted 23_tensorboard.ipynb.\n",
      "Converted 31_generator_learner.ipynb.\n",
      "Converted 32_critic_learner.ipynb.\n",
      "Converted 41_generator_pretraining.ipynb.\n",
      "Converted 42_critic_pretraining.ipynb.\n",
      "Converted 43_gan_training.ipynb.\n",
      "Converted EXPERIMENTS JOURNAL.ipynb.\n",
      "Converted augmentations.ipynb.\n",
      "Converted graphs-tests.ipynb.\n",
      "Converted sr reference.ipynb.\n"
     ]
    }
   ],
   "source": [
    "%nbdev_hide\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
